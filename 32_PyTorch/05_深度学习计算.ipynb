{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第5章 深度学习计算**\n",
    "\n",
    "@ Date: 2025-03-31<br>\n",
    "@ Author: Rui Zhu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 块(block)\n",
    "- 块: 可以描述单个层、由多个层组成的组件, 或整个模型本身\n",
    "- 使用块进行抽象的好处是: 可以将一些块组合成更大的组件\n",
    "- 编程的角度看, 块由类表示\n",
    "- pytorch中由Module(模块)表示块\n",
    "- 块必须提供的基本功能:\n",
    "    1. 输入数据作为其向前传播函数的输入(即`net(X)`用法, 实际上调用`net.__call__(X)`, 等价于`net.forward(X)`和一些内部操作)\n",
    "    2. 可以向前传播生成输出\n",
    "    3. 计算输出关于输入的梯度\n",
    "    4. 存储和访问所需参数\n",
    "    5. 可以初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 顺序块(nn.Sequential)\n",
    "- 按输入的各个层的顺序组成块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(20, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10),\n",
    ")\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0503, 0.6799, 0.6930, 0.3698, 0.9758, 0.4033, 0.8261, 0.4749, 0.5420,\n",
       "         0.5808, 0.3703, 0.6608, 0.6753, 0.0940, 0.5640, 0.7127, 0.4017, 0.2146,\n",
       "         0.2014, 0.1142],\n",
       "        [0.6852, 0.1228, 0.3651, 0.1883, 0.7049, 0.6834, 0.6705, 0.1299, 0.8873,\n",
       "         0.1896, 0.0326, 0.2032, 0.0810, 0.6917, 0.5289, 0.9858, 0.9337, 0.7571,\n",
       "         0.8813, 0.4752]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(2, 20)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0664, -0.0897, -0.3093, -0.1777, -0.0228,  0.0743, -0.1496, -0.0826,\n",
       "         -0.0046,  0.0664],\n",
       "        [-0.2194, -0.1063, -0.3389, -0.2302,  0.1186,  0.1686, -0.1135, -0.0627,\n",
       "          0.1381,  0.0696]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X)  # 调用模型获得输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0', Linear(in_features=20, out_features=256, bias=True)),\n",
       "             ('1', ReLU()),\n",
       "             ('2', Linear(in_features=256, out_features=10, bias=True))])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net._modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编写MLP的块\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)  # 隐藏层\n",
    "        self.out = nn.Linear(256, 10)  # 输出层\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0761,  0.2876,  0.1934, -0.0321, -0.0074, -0.0612,  0.2141, -0.0675,\n",
       "         -0.1378,  0.1146],\n",
       "        [ 0.1247,  0.3288,  0.0373, -0.1043, -0.0032, -0.0869,  0.2477,  0.0424,\n",
       "         -0.0485,  0.0034]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()  # 不用担心反向传播函数或初始化, 自动完成\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手动实现顺序块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0054, -0.1423,  0.0762,  0.1640, -0.0375, -0.1534, -0.1280, -0.0080,\n",
       "          0.1053,  0.0697],\n",
       "        [-0.1190, -0.0507,  0.0269,  0.2294, -0.0397, -0.2902, -0.0873, -0.0166,\n",
       "          0.2833,  0.0576]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            self._modules[str(idx)] = module\n",
    "    \n",
    "    def forward(self, X):\n",
    "        for block in self.children():\n",
    "            X = block(X)\n",
    "        return X\n",
    "net = MySequential(\n",
    "    nn.Linear(20, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10),\n",
    ")\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0', Linear(in_features=20, out_features=256, bias=True)),\n",
       "             ('1', ReLU()),\n",
       "             ('2', Linear(in_features=256, out_features=10, bias=True))])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net._modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在forward函数中执行代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0886, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)  # 随机权重\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        X = self.linear(X)\n",
    "\n",
    "        # 额外的控制\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()\n",
    "\n",
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 组合块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0520, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(20, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(\n",
    "    NestMLP(), \n",
    "    nn.Linear(16, 20),\n",
    "    FixedHiddenMLP()\n",
    ")\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 参数管理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2293],\n",
       "        [-0.1451]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建MLP\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4, 8), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(8, 1)\n",
    ")\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0', Linear(in_features=4, out_features=8, bias=True)),\n",
       "             ('1', ReLU()),\n",
       "             ('2', Linear(in_features=8, out_features=1, bias=True))])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net._modules  # 查看模型的所有层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ 0.4699, -0.3585, -0.1865,  0.3454],\n",
       "                      [ 0.4991, -0.1852,  0.2873, -0.1153],\n",
       "                      [-0.3224,  0.3879,  0.4573,  0.1807],\n",
       "                      [ 0.0139, -0.0044,  0.4673,  0.4174],\n",
       "                      [-0.1147,  0.3683,  0.3931,  0.4315],\n",
       "                      [-0.0214,  0.2101, -0.4896,  0.2749],\n",
       "                      [ 0.0464,  0.4439,  0.4932,  0.1798],\n",
       "                      [-0.3920, -0.1576,  0.3213,  0.4544]])),\n",
       "             ('bias',\n",
       "              tensor([-0.2147,  0.4471, -0.0628,  0.3382,  0.1391,  0.3587,  0.1207, -0.4256]))])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].state_dict()  # 查看模型的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.2147,  0.4471, -0.0628,  0.3382,  0.1391,  0.3587,  0.1207, -0.4256],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].bias  # 查看模型的偏置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2147,  0.4471, -0.0628,  0.3382,  0.1391,  0.3587,  0.1207, -0.4256])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].bias.data  # 查看模型的偏置数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.grad == None  # 查看模型的权重梯度\n",
    "# ! 因为没有调用反向传播函数, 所以梯度为None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4699, -0.3585, -0.1865,  0.3454],\n",
       "        [ 0.4991, -0.1852,  0.2873, -0.1153],\n",
       "        [-0.3224,  0.3879,  0.4573,  0.1807],\n",
       "        [ 0.0139, -0.0044,  0.4673,  0.4174],\n",
       "        [-0.1147,  0.3683,  0.3931,  0.4315],\n",
       "        [-0.0214,  0.2101, -0.4896,  0.2749],\n",
       "        [ 0.0464,  0.4439,  0.4932,  0.1798],\n",
       "        [-0.3920, -0.1576,  0.3213,  0.4544]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['0.weight']  # 一种访问模型参数的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight torch.Size([8, 4])\n",
      "0.bias torch.Size([8])\n",
      "2.weight torch.Size([1, 8])\n",
      "2.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# 访问所有参数\n",
    "for name, param in net.named_parameters():\n",
    "    print(name, param.size())  # 打印参数名称和大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight torch.Size([8, 4])\n",
      "bias torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# 访问所有参数\n",
    "for name, param in net[0].named_parameters():\n",
    "    print(name, param.size())  # 打印参数名称和大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 嵌套块的参数访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1392],\n",
       "        [0.1391]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(4, 8), \n",
    "        nn.ReLU(), \n",
    "        nn.Linear(8, 4), \n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f\"block {i}\", block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(\n",
    "    block2(), \n",
    "    nn.Linear(4, 1)\n",
    ")\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (block 0): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (block 1): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (block 2): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (block 3): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3418, -0.1416, -0.2427, -0.2503,  0.0218,  0.2951, -0.4175, -0.2851])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet[0][1][0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数初始化\n",
    "- 默认情况下, pytorch会根据一个范围均匀地初始化权重和偏置矩阵, 这个范围是根据输入维度和输出维度计算出的\n",
    "- 对于nn.Linear, 默认的初始化方法是Kaiming (He) 初始化: 使用均匀分布, 初始化范围由输入单元数决定$Var(W) = \\frac{2}{n_{in}}$, bias为0初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 内置初始化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0134,  0.0102, -0.0028,  0.0220]), tensor(0.))"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):\n",
    "    \"\"\"\n",
    "    使用正态分布初始化模型参数\n",
    "    \"\"\"\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]  # 查看模型的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    \"\"\"\n",
    "    使用常数初始化模型参数\n",
    "    \"\"\"\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0], net[0].bias.data[0]  # 查看模型的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2316, -0.4256, -0.1055, -0.1780],\n",
      "        [ 0.5858, -0.6287, -0.2829, -0.4852],\n",
      "        [ 0.1387, -0.1904,  0.3128, -0.4191],\n",
      "        [-0.2791, -0.5202, -0.2405, -0.5866],\n",
      "        [ 0.3185, -0.1187, -0.4437, -0.6686],\n",
      "        [ 0.5539, -0.4810,  0.4688, -0.1337],\n",
      "        [ 0.0854, -0.4808, -0.4596,  0.5794],\n",
      "        [-0.6199,  0.5872, -0.0100,  0.3264]])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "## 不同的层使用不同的初始化\n",
    "def init_xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "\n",
    "net[0].apply(init_xavier)\n",
    "net[2].apply(init_42)\n",
    "\n",
    "print(net[0].weight.data)\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init ('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "Init ('weight', torch.Size([1, 8])) ('bias', torch.Size([1]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0000, -7.6155, -9.7038, -0.0000],\n",
       "        [-8.3351,  0.0000,  8.7073,  6.6785],\n",
       "        [ 7.5402,  0.0000, -0.0000,  0.0000],\n",
       "        [ 6.2078, -6.2822, -0.0000, -0.0000],\n",
       "        [-5.4853, -0.0000, -0.0000,  0.0000],\n",
       "        [ 0.0000,  5.7375, -5.0802, -0.0000],\n",
       "        [ 5.1756,  5.6453, -0.0000, -7.7877],\n",
       "        [-8.1221, -7.7689,  0.0000,  8.5102]], requires_grad=True)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.size()) for name, param in m.named_parameters()])\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 修改权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000, -7.6155, -9.7038, -0.0000],\n",
       "        [-8.3351,  0.0000,  8.7073,  6.6785],\n",
       "        [ 7.5402,  0.0000, -0.0000,  0.0000],\n",
       "        [ 6.2078, -6.2822, -0.0000, -0.0000],\n",
       "        [-5.4853, -0.0000, -0.0000,  0.0000],\n",
       "        [ 0.0000,  5.7375, -5.0802, -0.0000],\n",
       "        [ 5.1756,  5.6453, -0.0000, -7.7877],\n",
       "        [-8.1221, -7.7689,  0.0000,  8.5102]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "net[0].weight.data += 1\n",
    "net[0].weight.data[0, 0] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[42.0000, -6.6155, -8.7038,  1.0000],\n",
       "        [-7.3351,  1.0000,  9.7073,  7.6785],\n",
       "        [ 8.5402,  1.0000,  1.0000,  1.0000],\n",
       "        [ 7.2078, -5.2822,  1.0000,  1.0000],\n",
       "        [-4.4853,  1.0000,  1.0000,  1.0000],\n",
       "        [ 1.0000,  6.7375, -4.0802,  1.0000],\n",
       "        [ 6.1756,  6.6453,  1.0000, -6.7877],\n",
       "        [-7.1221, -6.7689,  1.0000,  9.5102]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 共享层的参数绑定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=8, out_features=8, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=8, out_features=8, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4, 8), nn.ReLU(), \n",
    "    shared, nn.ReLU(),\n",
    "    shared, nn.ReLU(),\n",
    "    nn.Linear(8, 1)\n",
    ")\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0936],\n",
       "        [0.1038]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "# 检查参数是否相同\n",
    "print(net[2].weight.data == net[4].weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "# 确保共享层实际上是同一个对象, 而不是只是有相同的值\n",
    "net[2].weight.data[0, 0] = 100\n",
    "print(net[2].weight.data == net[4].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
