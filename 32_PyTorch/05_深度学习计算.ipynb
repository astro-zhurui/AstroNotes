{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第5章 深度学习计算**\n",
    "\n",
    "@ Date: 2025-03-31<br>\n",
    "@ Author: Rui Zhu<br>\n",
    "@ Note: 使用astro环境以保证使用最新的pytorch版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 块(block)\n",
    "- 块: 可以描述单个层、由多个层组成的组件, 或整个模型本身\n",
    "- 使用块进行抽象的好处是: 可以将一些块组合成更大的组件\n",
    "- 编程的角度看, 块由类表示\n",
    "- pytorch中由Module(模块)表示块\n",
    "- 块必须提供的基本功能:\n",
    "    1. 输入数据作为其向前传播函数的输入(即`net(X)`用法, 实际上调用`net.__call__(X)`, 等价于`net.forward(X)`和一些内部操作)\n",
    "    2. 可以向前传播生成输出\n",
    "    3. 计算输出关于输入的梯度\n",
    "    4. 存储和访问所需参数\n",
    "    5. 可以初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 顺序块(nn.Sequential)\n",
    "- 按输入的各个层的顺序组成块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(20, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10),\n",
    ")\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1352, 0.5201, 0.8918, 0.0411, 0.8182, 0.0069, 0.5662, 0.3699, 0.0806,\n",
       "         0.4544, 0.0214, 0.0541, 0.0650, 0.7913, 0.1111, 0.1528, 0.0482, 0.1007,\n",
       "         0.9258, 0.7067],\n",
       "        [0.6322, 0.1852, 0.8061, 0.1109, 0.0708, 0.7868, 0.9678, 0.1769, 0.1047,\n",
       "         0.5166, 0.4200, 0.6454, 0.6484, 0.8026, 0.7675, 0.7060, 0.4776, 0.1926,\n",
       "         0.9065, 0.0157]])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(2, 20)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0018, -0.0990,  0.0108,  0.1761,  0.3643,  0.0541, -0.0111,  0.0722,\n",
       "          0.0359, -0.0259],\n",
       "        [-0.0936, -0.1328,  0.0822,  0.0439,  0.3430, -0.0354,  0.0339,  0.1095,\n",
       "         -0.0946,  0.0232]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X)  # 调用模型获得输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': Linear(in_features=20, out_features=256, bias=True),\n",
       " '1': ReLU(),\n",
       " '2': Linear(in_features=256, out_features=10, bias=True)}"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net._modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编写MLP的块\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)  # 隐藏层\n",
    "        self.out = nn.Linear(256, 10)  # 输出层\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0269, -0.0152,  0.0758,  0.1535,  0.1294,  0.0723, -0.0340, -0.0219,\n",
       "         -0.0129, -0.0445],\n",
       "        [-0.0626,  0.0412,  0.0689,  0.2269,  0.2851, -0.0695,  0.0508, -0.0397,\n",
       "         -0.0174,  0.0271]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()  # 不用担心反向传播函数或初始化, 自动完成\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手动实现顺序块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1050,  0.0060,  0.1853, -0.0542,  0.0855, -0.1577,  0.1147,  0.0351,\n",
       "          0.1566, -0.0883],\n",
       "        [-0.0170,  0.0967,  0.0883, -0.0262,  0.0456, -0.1873, -0.0819, -0.0151,\n",
       "          0.2610, -0.1860]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            self._modules[str(idx)] = module\n",
    "    \n",
    "    def forward(self, X):\n",
    "        for block in self.children():\n",
    "            X = block(X)\n",
    "        return X\n",
    "net = MySequential(\n",
    "    nn.Linear(20, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10),\n",
    ")\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': Linear(in_features=20, out_features=256, bias=True),\n",
       " '1': ReLU(),\n",
       " '2': Linear(in_features=256, out_features=10, bias=True)}"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net._modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在forward函数中执行代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1555, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)  # 随机权重\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        X = self.linear(X)\n",
    "\n",
    "        # 额外的控制\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()\n",
    "\n",
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 组合块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0723, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(20, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(\n",
    "    NestMLP(), \n",
    "    nn.Linear(16, 20),\n",
    "    FixedHiddenMLP()\n",
    ")\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 参数管理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3976],\n",
       "        [0.3495]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建MLP\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4, 8), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(8, 1)\n",
    ")\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': Linear(in_features=4, out_features=8, bias=True),\n",
       " '1': ReLU(),\n",
       " '2': Linear(in_features=8, out_features=1, bias=True)}"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net._modules  # 查看模型的所有层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ 0.4308,  0.0258, -0.4830, -0.4100],\n",
       "                      [ 0.1067, -0.3336,  0.1812,  0.3307],\n",
       "                      [ 0.0948,  0.1487, -0.4548, -0.4789],\n",
       "                      [ 0.2638, -0.1602,  0.4781, -0.2466],\n",
       "                      [ 0.4083, -0.2553,  0.4273, -0.0307],\n",
       "                      [ 0.1976, -0.4209,  0.3862,  0.3559],\n",
       "                      [-0.1568, -0.0467, -0.0026,  0.4222],\n",
       "                      [ 0.2289,  0.4882,  0.3886,  0.3086]])),\n",
       "             ('bias',\n",
       "              tensor([ 0.0929,  0.4731, -0.4559, -0.0874, -0.0534,  0.0716, -0.3312,  0.0290]))])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].state_dict()  # 查看模型的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.0929,  0.4731, -0.4559, -0.0874, -0.0534,  0.0716, -0.3312,  0.0290],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].bias  # 查看模型的偏置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0929,  0.4731, -0.4559, -0.0874, -0.0534,  0.0716, -0.3312,  0.0290])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].bias.data  # 查看模型的偏置数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.grad == None  # 查看模型的权重梯度\n",
    "# ! 因为没有调用反向传播函数, 所以梯度为None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4308,  0.0258, -0.4830, -0.4100],\n",
       "        [ 0.1067, -0.3336,  0.1812,  0.3307],\n",
       "        [ 0.0948,  0.1487, -0.4548, -0.4789],\n",
       "        [ 0.2638, -0.1602,  0.4781, -0.2466],\n",
       "        [ 0.4083, -0.2553,  0.4273, -0.0307],\n",
       "        [ 0.1976, -0.4209,  0.3862,  0.3559],\n",
       "        [-0.1568, -0.0467, -0.0026,  0.4222],\n",
       "        [ 0.2289,  0.4882,  0.3886,  0.3086]])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['0.weight']  # 一种访问模型参数的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight torch.Size([8, 4])\n",
      "0.bias torch.Size([8])\n",
      "2.weight torch.Size([1, 8])\n",
      "2.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# 访问所有参数\n",
    "for name, param in net.named_parameters():\n",
    "    print(name, param.size())  # 打印参数名称和大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight torch.Size([8, 4])\n",
      "bias torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# 访问所有参数\n",
    "for name, param in net[0].named_parameters():\n",
    "    print(name, param.size())  # 打印参数名称和大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 嵌套块的参数访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1964],\n",
       "        [0.1965]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(4, 8), \n",
    "        nn.ReLU(), \n",
    "        nn.Linear(8, 4), \n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f\"block {i}\", block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(\n",
    "    block2(), \n",
    "    nn.Linear(4, 1)\n",
    ")\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (block 0): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (block 1): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (block 2): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (block 3): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0645,  0.2344, -0.2525,  0.4483, -0.2970,  0.0964, -0.2381,  0.1010])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet[0][1][0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数初始化\n",
    "- 默认情况下, pytorch会根据一个范围均匀地初始化权重和偏置矩阵, 这个范围是根据输入维度和输出维度计算出的\n",
    "- 对于nn.Linear, 默认的初始化方法是Kaiming (He) 初始化: 使用均匀分布, 初始化范围由输入单元数决定$Var(W) = \\frac{2}{n_{in}}$, bias为0初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 内置初始化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0122, -0.0087, -0.0002,  0.0211]), tensor(0.))"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):\n",
    "    \"\"\"\n",
    "    使用正态分布初始化模型参数\n",
    "    \"\"\"\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]  # 查看模型的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    \"\"\"\n",
    "    使用常数初始化模型参数\n",
    "    \"\"\"\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0], net[0].bias.data[0]  # 查看模型的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6014,  0.0755,  0.5984, -0.4266],\n",
      "        [-0.4349,  0.4243, -0.5063, -0.0501],\n",
      "        [-0.6736, -0.4159,  0.3296, -0.6554],\n",
      "        [-0.0694,  0.4402,  0.3145,  0.4364],\n",
      "        [ 0.4501, -0.0775, -0.0040, -0.5507],\n",
      "        [-0.3460, -0.6581,  0.1722, -0.1373],\n",
      "        [-0.5771, -0.0448, -0.6864,  0.0262],\n",
      "        [-0.2901, -0.4147, -0.0960,  0.2585]])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "## 不同的层使用不同的初始化\n",
    "def init_xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "\n",
    "net[0].apply(init_xavier)\n",
    "net[2].apply(init_42)\n",
    "\n",
    "print(net[0].weight.data)\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init ('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "Init ('weight', torch.Size([1, 8])) ('bias', torch.Size([1]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 5.5724, -0.0000,  9.9922, -8.6631],\n",
       "        [ 6.4397,  0.0000, -0.0000,  6.6075],\n",
       "        [ 0.0000, -0.0000,  0.0000, -8.8608],\n",
       "        [-5.9956, -0.0000,  6.7102, -5.7517],\n",
       "        [ 6.1070,  0.0000,  5.0100, -0.0000],\n",
       "        [ 7.0016, -6.0194, -8.1283,  5.4038],\n",
       "        [-0.0000, -0.0000, -0.0000,  0.0000],\n",
       "        [-8.4616,  0.0000,  0.0000,  7.0875]], requires_grad=True)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.size()) for name, param in m.named_parameters()])\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 修改权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.5724, -0.0000,  9.9922, -8.6631],\n",
       "        [ 6.4397,  0.0000, -0.0000,  6.6075],\n",
       "        [ 0.0000, -0.0000,  0.0000, -8.8608],\n",
       "        [-5.9956, -0.0000,  6.7102, -5.7517],\n",
       "        [ 6.1070,  0.0000,  5.0100, -0.0000],\n",
       "        [ 7.0016, -6.0194, -8.1283,  5.4038],\n",
       "        [-0.0000, -0.0000, -0.0000,  0.0000],\n",
       "        [-8.4616,  0.0000,  0.0000,  7.0875]])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "net[0].weight.data += 1\n",
    "net[0].weight.data[0, 0] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[42.0000,  1.0000, 10.9922, -7.6631],\n",
       "        [ 7.4397,  1.0000,  1.0000,  7.6075],\n",
       "        [ 1.0000,  1.0000,  1.0000, -7.8608],\n",
       "        [-4.9956,  1.0000,  7.7102, -4.7517],\n",
       "        [ 7.1070,  1.0000,  6.0100,  1.0000],\n",
       "        [ 8.0016, -5.0194, -7.1283,  6.4038],\n",
       "        [ 1.0000,  1.0000,  1.0000,  1.0000],\n",
       "        [-7.4616,  1.0000,  1.0000,  8.0875]])"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 共享层的参数绑定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=8, out_features=8, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=8, out_features=8, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4, 8), nn.ReLU(), \n",
    "    shared, nn.ReLU(),\n",
    "    shared, nn.ReLU(),\n",
    "    nn.Linear(8, 1)\n",
    ")\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0932],\n",
       "        [-0.0880]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "# 检查参数是否相同\n",
    "print(net[2].weight.data == net[4].weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "# 确保共享层实际上是同一个对象, 而不是只是有相同的值\n",
    "net[2].weight.data[0, 0] = 100\n",
    "print(net[2].weight.data == net[4].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 自定义层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不带参数的层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return X - X.mean()\n",
    "    \n",
    "layer = CenteredLayer()\n",
    "layer(torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.5879e-09, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())\n",
    "Y = net(torch.rand(4, 8))\n",
    "Y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 带参数的层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.7028, -1.1039,  0.2312],\n",
       "         [-0.0592,  0.8001,  0.5042],\n",
       "         [ 1.5984, -0.3153, -0.8094],\n",
       "         [-0.3796,  0.1617,  0.0610],\n",
       "         [ 1.1833,  0.7279, -1.1483]]),\n",
       " tensor([0., 0., 0.]))"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自定义版本的全连接层\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_units, unints):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units, unints))\n",
    "        self.bias = nn.Parameter(torch.zeros(unints))\n",
    "    def forward(self, X):\n",
    "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
    "        return F.relu(linear)\n",
    "\n",
    "linear = MyLinear(5, 3)\n",
    "linear.weight.data, linear.bias.data  # 查看模型的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7795],\n",
       "        [1.8713]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(MyLinear(64, 8), nn.Linear(8, 1))\n",
    "net(torch.rand(2, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): MyLinear()\n",
       "  (1): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 读写文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "dir_save = Path(\"/Users/rui/Code/Astronote/32_PyTorch/data/chapter5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存和加载张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(4)\n",
    "torch.save(x, dir_save / 'x-file')  # 保存进一个二进制文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = torch.load(dir_save / 'x-file')  # 从文件中加载\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存一个张量为元素的列表\n",
    "y = torch.zeros(4)\n",
    "torch.save([x, y], dir_save / 'xy-file')  # 保存进一个二进制文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2, y2 = torch.load(dir_save / 'xy-file')  # 从文件中加载\n",
    "x2, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存字典\n",
    "mydict = {'x': x, 'y': y}\n",
    "torch.save(mydict, dir_save / 'mydict')  # 保存进一个二进制文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(dir_save / 'mydict')  # 从文件中加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存和加载模型参数\n",
    "- 保存的是模型参数, 而不是模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)  # 隐藏层\n",
    "        self.out = nn.Linear(256, 10)  # 输出层\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))\n",
    "    \n",
    "net = MLP()\n",
    "X = torch.rand(2, 20)\n",
    "Y = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型参数\n",
    "torch.save(net.state_dict(), dir_save / 'mlp.params')  # 保存进一个二进制文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (out): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_MLP = MLP()\n",
    "# 加载模型参数\n",
    "new_MLP.load_state_dict(torch.load(dir_save / 'mlp.params'))\n",
    "new_MLP.eval()  # 切换到评估模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_MLP(X) == Y  # 相同的模型参数, 所以输出相同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# GPU\n",
    "- 默认, 张量在内存中创建, 在CPU中计算\n",
    "- 计算必须在同一个计算设备上\n",
    "- 使用GPU会提速, 但CPU和GPU之间的数据传输比计算慢的多\n",
    "- print操作或将tensor转换成numpy, 数据会复制进内存, 造成额外的数据传输开销"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查询和指定计算设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch中的计算设备表示\n",
    "torch.device('cpu')  # CPU\n",
    "torch.device('cuda')  # GPU\n",
    "torch.device('cuda:0')  # 第一个GPU\n",
    "torch.device('cuda:1')  # 第二个GPU\n",
    "torch.device('mps')  # Apple Silicon GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查询可用GPU数量\n",
    "torch.cuda.device_count()  # 可用GPU数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mps.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 优先使用GPU的代码\n",
    "def try_gpu(i=0):\n",
    "    if torch.cuda.device_count() > i:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "try_gpu()  # 返回可用的GPU设备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量与GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]], device='mps:0')"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建张量时制定计算设备\n",
    "X = torch.ones(2, 3, device='mps')\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4954, 0.8590, 0.6945],\n",
       "        [0.6987, 0.3429, 0.7869]])"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.rand(2, 3, device='cpu')\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4954, 1.8590, 1.6945],\n",
       "        [1.6987, 1.3429, 1.7869]], device='mps:0')"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_mps = Y.to('mps')  # 将Y转移到GPU\n",
    "Y_mps + X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络与GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.7 ms, sys: 334 ms, total: 336 ms\n",
      "Wall time: 337 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0649, -0.0902,  0.0179,  ..., -0.0691, -0.2314, -0.0428],\n",
       "        [ 0.0025, -0.0709,  0.1100,  ..., -0.0938, -0.2828, -0.1028],\n",
       "        [-0.0916,  0.0171, -0.0261,  ..., -0.1459, -0.1277,  0.0034],\n",
       "        ...,\n",
       "        [ 0.0227, -0.0371, -0.0042,  ..., -0.0379, -0.0863,  0.0070],\n",
       "        [ 0.1797, -0.0912, -0.0589,  ..., -0.1161, -0.2025, -0.0245],\n",
       "        [ 0.1110, -0.0159,  0.0244,  ..., -0.1549, -0.2639,  0.0791]],\n",
       "       device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "X = torch.rand(10000, 30, device='mps')\n",
    "net = nn.Sequential(nn.Linear(30, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net = net.to('mps')  # 将模型转移到GPU\n",
    "net(X)  # 在GPU上运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data.device  # 查看模型的参数所在设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.6 ms, sys: 9.78 ms, total: 22.3 ms\n",
      "Wall time: 5.75 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0011, -0.2461,  0.0888,  ...,  0.0243,  0.0034, -0.1190],\n",
       "        [ 0.0667, -0.1309,  0.1439,  ...,  0.0574, -0.0008, -0.0951],\n",
       "        [ 0.0304, -0.1504,  0.1026,  ...,  0.1787, -0.0491, -0.0970],\n",
       "        ...,\n",
       "        [-0.0030, -0.0071,  0.0610,  ...,  0.0751, -0.0715, -0.1405],\n",
       "        [ 0.0564, -0.1127,  0.0864,  ...,  0.1331, -0.0869, -0.0163],\n",
       "        [ 0.1161, -0.2570,  0.1685,  ...,  0.0620, -0.0645, -0.1016]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "X = torch.rand(10000, 30, device='cpu')\n",
    "net = nn.Sequential(nn.Linear(30, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
