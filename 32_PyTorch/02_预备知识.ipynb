{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pytorch实用技能**\n",
    "\n",
    "@ Follow: \"动手学深度学习-第二章 预备知识\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "dir_chapter = Path(\"/Users/rui/Code/Astronote/32_PyTorch/data/chapter2\")\n",
    "dir_chapter.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 基本数据结构-Tensor\n",
    "- Tensor与numpy array非常相似, 但有2个新功能\n",
    "- Tensor支持GPU, 支持自动微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[[ 0.3124],\n",
      "         [ 0.3354]],\n",
      "\n",
      "        [[ 2.6748],\n",
      "         [ 0.4485]],\n",
      "\n",
      "        [[-0.1018],\n",
      "         [-1.1281]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(12)  # 创建一个行向量\n",
    "X_all0 = torch.zeros((2, 3))  # 创建一个2行3列的矩阵，元素全为0\n",
    "X_all1 = torch.ones((2, 3))  # 创建一个2行3列的矩阵，元素全为1\n",
    "X_rand = torch.randn(3, 2, 1)  # 创建一个2行3列的矩阵，元素为随机数\n",
    "\n",
    "print(x)\n",
    "print(X_all0)\n",
    "print(X_all1)\n",
    "print(X_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 1, 4, 3],\n",
      "        [1, 2, 3, 4],\n",
      "        [4, 3, 2, 1]])\n",
      "tensor([[2, 1, 4, 3],\n",
      "        [1, 2, 3, 4],\n",
      "        [4, 3, 2, 1]])\n"
     ]
    }
   ],
   "source": [
    "ls = [[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]\n",
    "X = torch.tensor(ls)  # 通过列表创建张量\n",
    "\n",
    "arr = np.array(ls)\n",
    "X_arr = torch.from_numpy(arr)  # 通过numpy数组创建张量\n",
    "\n",
    "print(X)\n",
    "print(X_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看Tensor的属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12])\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)  # 查看形状, 即沿每个轴的元素数量\n",
    "print(x.numel())  # 查看元素数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor的操作与运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更改张量形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "X = x.reshape(3, 4)  # 将行向量x的形状改为(3, 4), 即3行4列的矩阵\n",
    "X1 = x.reshape(-1, 3)  # 指定形状为3列, 行数自动推断\n",
    "\n",
    "print(X)\n",
    "print(X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按元素运算\n",
    "- 按元素位置单独操作或运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.,  4.,  6., 10.]),\n",
       " tensor([-1.,  0.,  2.,  6.]),\n",
       " tensor([ 2.,  4.,  8., 16.]),\n",
       " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
       " tensor([ 1.,  4., 16., 64.]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "x + y, x - y, x * y, x / y, x ** y  # 加减乘除乘方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(x)  # 指数运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "广播机制: 即使两个张量的形状不同, 也可以执行元素操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[0, 1]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3).reshape((3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "tensor([[2., 1., 4., 3.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [4., 3., 2., 1.]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.arange(12, dtype=torch.float32).reshape((3, 4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [ 2.,  1.,  4.,  3.],\n",
       "        [ 1.,  2.,  3.,  4.],\n",
       "        [ 4.,  3.,  2.,  1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((X, Y), dim=0)  # 沿行（轴0）拼接X和Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((X, Y), dim=1)  # 沿列（轴1）拼接X和Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 节省内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(X): 5064406064\n",
      "id(X): 5064403504\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "直接赋值相同的变量名仍然分配了不同的内存\n",
    "\"\"\"\n",
    "X = torch.arange(12, dtype=torch.float32).reshape((3, 4))\n",
    "before = id(X)\n",
    "print(f\"id(X): {before}\")\n",
    "\n",
    "X = X + 1\n",
    "print(f\"id(X): {id(X)}\")  # id(X)与before不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(X): 5064394864\n",
      "id(X): 5064394864\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "使用切片表示法覆盖内存\n",
    "\"\"\"\n",
    "X = torch.arange(12, dtype=torch.float32).reshape((3, 4))\n",
    "before = id(X)\n",
    "print(f\"id(X): {before}\")\n",
    "\n",
    "X[:] = X + 1\n",
    "print(f\"id(X): {id(X)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(X): 5064607536\n",
      "id(X): 5064607536\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "使用 += 覆盖内存\n",
    "\"\"\"\n",
    "X = torch.arange(12, dtype=torch.float32).reshape((3, 4))\n",
    "before = id(X)\n",
    "print(f\"id(X): {before}\")\n",
    "\n",
    "X += 1\n",
    "print(f\"id(X): {id(X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor和Numpy array转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'numpy.ndarray'>\n",
      "5064408464 5043389232\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "[[ 0.  1.  2.  3.]\n",
      " [ 4.  5.  6.  7.]\n",
      " [ 8.  9. 10. 11.]]\n",
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.]])\n",
      "[[ 1.  2.  3.  4.]\n",
      " [ 5.  6.  7.  8.]\n",
      " [ 9. 10. 11. 12.]]\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(12, dtype=torch.float32).reshape((3, 4))\n",
    "B = A.numpy()\n",
    "print(type(A), type(B))\n",
    "print(id(A), id(B))  # 内存地址不同\n",
    "print(A)\n",
    "print(B)\n",
    "\n",
    "# tensor和numpy数组共享内存, 修改一个会影响另一个(仅限就地操作, 如 +=, A[:])\n",
    "B[:] = B + 1\n",
    "print(A)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 通过Pandas读取数据\n",
    "- 步骤: 数据读取、缺失值处理、转换成Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "创建测试数据文件\n",
    "\"\"\"\n",
    "data_file = dir_chapter / \"house_tiny.csv\"\n",
    "with open(data_file, 'w') as f:\n",
    "    f.write('NumRooms,Alley,Price\\n')  # 列名\n",
    "    f.write('NA,Pave,127500\\n')  # 每行表示一个数据样本\n",
    "    f.write('2,NA,106000\\n')\n",
    "    f.write('4,NA,178100\\n')\n",
    "    f.write('NA,NA,140000\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms Alley   Price\n",
      "0       NaN  Pave  127500\n",
      "1       2.0   NaN  106000\n",
      "2       4.0   NaN  178100\n",
      "3       NaN   NaN  140000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "load the data\n",
    "\"\"\"\n",
    "data = pd.read_csv(data_file)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NumRooms</th>\n",
       "      <th>Alley_Pave</th>\n",
       "      <th>Alley_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NumRooms  Alley_Pave  Alley_nan\n",
       "0       3.0           1          0\n",
       "1       2.0           0          1\n",
       "2       4.0           0          1\n",
       "3       3.0           0          1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "replace the missing value\n",
    "\"\"\"\n",
    "# 均值填充数值\n",
    "inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]\n",
    "inputs['NumRooms'] = inputs['NumRooms'].fillna(inputs['NumRooms'].mean())\n",
    "\n",
    "# 单一类别分类缺失处理\n",
    "inputs = pd.get_dummies(inputs, dummy_na=True)\n",
    "# bool转换\n",
    "inputs['Alley_Pave'] = inputs['Alley_Pave'].astype('int')\n",
    "inputs['Alley_nan'] = inputs['Alley_nan'].astype('int')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3., 1., 0.],\n",
       "         [2., 0., 1.],\n",
       "         [4., 0., 1.],\n",
       "         [3., 0., 1.]], dtype=torch.float64),\n",
       " tensor([127500, 106000, 178100, 140000]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "数据转换成Tensor\n",
    "\"\"\"\n",
    "X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)\n",
    "X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 线性代数的pytorch实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量\n",
    "- 列向量是向量的默认方向"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵\n",
    "- 在表格数据集中, 一般将每个样本作为矩阵中的行向量, 这样沿着张量的最外轴可以访问或遍历小批量样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15],\n",
      "        [16, 17, 18, 19]])\n",
      "Transpose of A: \n",
      "tensor([[ 0,  4,  8, 12, 16],\n",
      "        [ 1,  5,  9, 13, 17],\n",
      "        [ 2,  6, 10, 14, 18],\n",
      "        [ 3,  7, 11, 15, 19]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(20).reshape(5, 4)\n",
    "A_trans = A.T\n",
    "\n",
    "print(A)\n",
    "print(f\"Transpose of A: \\n{A_trans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 哈达玛积(Hadamard product): 两个矩阵的按元素乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "tensor([[ 0.,  2.,  4.,  6.],\n",
      "        [ 8., 10., 12., 14.],\n",
      "        [16., 18., 20., 22.],\n",
      "        [24., 26., 28., 30.],\n",
      "        [32., 34., 36., 38.]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "B = A.clone()\n",
    "\n",
    "print(A)\n",
    "print(A + B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对所有元素求和: 190.0\n",
      "沿着行方向对列求和: tensor([40., 45., 50., 55.])\n",
      "沿着列方向对行求和: tensor([ 6., 22., 38., 54., 70.])\n",
      "沿着行列对矩阵求和: 190.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"对所有元素求和: {A.sum()}\")\n",
    "print(f\"沿着行方向对列求和: {A.sum(axis=0)}\")\n",
    "print(f\"沿着列方向对行求和: {A.sum(axis=1)}\")\n",
    "print(f\"沿着行列对矩阵求和: {A.sum(axis=[0, 1])}\")  # 等价于A.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对所有元素求和: \n",
      "tensor([[40., 45., 50., 55.]])\n",
      "沿着行方向对列求和: \n",
      "tensor([[ 6.],\n",
      "        [22.],\n",
      "        [38.],\n",
      "        [54.],\n",
      "        [70.]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "以上求和后都是向量, 如果需要保持矩阵形状, 可以指定keepdims=True\n",
    "\"\"\"\n",
    "print(f\"对所有元素求和: \\n{A.sum(axis=0, keepdims=True)}\")\n",
    "print(f\"沿着行方向对列求和: \\n{A.sum(axis=1, keepdims=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [3., 3., 3., 3.]])\n",
      "tensor([[1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "计算累计求和\n",
    "\"\"\"\n",
    "A = torch.ones(3, 4)\n",
    "print(A)\n",
    "print(A.cumsum(axis=0))\n",
    "print(A.cumsum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 点积\n",
    "- 对应元素相乘再求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([0., 1., 2., 3.])\n",
      "y: tensor([1., 1., 1., 1.])\n",
      "dot: 6.0\n",
      "sum(x * y): 6.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4, dtype=torch.float32)\n",
    "y = torch.ones(4, dtype=torch.float32)\n",
    "\n",
    "dot = torch.dot(x, y)\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")\n",
    "print(f\"dot: {dot}\")\n",
    "print(f\"sum(x * y): {torch.sum(x * y)}\")  # 等价于torch.dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵-向量积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "tensor([0., 1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "x = torch.arange(4, dtype=torch.float32)\n",
    "print(A)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 14.,  38.,  62.,  86., 110.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mv(A, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵-矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [2, 3]])\n",
      "tensor([[0, 1],\n",
      "        [2, 3]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(4).reshape(2, 2)\n",
    "B = torch.arange(4).reshape(2, 2)\n",
    "print(A)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  3],\n",
       "        [ 6, 11]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 范数\n",
    "- 非正式的说, 范数表示张量的长度\n",
    "- 范数$L_P$的一般形式:<br>\n",
    "    $||x||_p = (\\Sigma^n_{i=1}|x_i|^P)^{1/P}$\n",
    "- L2范数即欧式距离: $||x||_2 = \\sqrt{\\Sigma^n_{i=1}|x_i|^2}$\n",
    "- L1范数即元素的绝对值求和, 与L2范数相比, L1范数受异常值影响更小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1.],\n",
      "        [2., 3.]], dtype=torch.float16)\n",
      "tensor(6., dtype=torch.float16)\n",
      "tensor(3.7422, dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(4, dtype=torch.float16).reshape(2, -1)\n",
    "print(A)\n",
    "print(torch.norm(A, p=1))  # L1范数\n",
    "print(torch.norm(A, p=2))  # L2范数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 自动微分\n",
    "- pytorch可以自动计算导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.], dtype=torch.float16, requires_grad=True)\n",
      "tensor(28., dtype=torch.float16, grad_fn=<MulBackward0>)\n",
      "tensor([ 0.,  4.,  8., 12.], dtype=torch.float16)\n",
      "tensor([True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4, dtype=torch.float16, requires_grad=True)\n",
    "# 定义函数形式\n",
    "y = 2 * torch.dot(x, x)\n",
    "# 反向传播计算函数y关于每个参数x的梯度\n",
    "y.backward()\n",
    "print(x)\n",
    "print(y)\n",
    "print(x.grad)\n",
    "print(x.grad == 4 * x)  # 与手动计算的梯度值比较"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
