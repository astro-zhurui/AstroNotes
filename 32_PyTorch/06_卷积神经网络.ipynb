{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f3fd262",
   "metadata": {},
   "source": [
    "**卷积神经网络 (Convolutional Nerual Network, CNN)**\n",
    "\n",
    "@ Date: 2025-04-06<br>\n",
    "@ Author: Rui Zhu<br>\n",
    "@ Note: <br> \n",
    "    1. CNN是一类强大的、为处理图像数据而设计的神经网络<br>\n",
    "    2. CNN需要的参数少于全连接架构的网络, 而且卷积容易使用GPU平行计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc51da8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261d2053",
   "metadata": {},
   "source": [
    "---\n",
    "# 基本概念\n",
    "- 图像中有丰富的结构, CNN通过卷积层提取了图像的结构特征\n",
    "- 图像中的结构特性:\n",
    "    1. 平移不变性(Translation Invariance): 图像中的物体可以出现在任意位置, 识别物体不应关注其在图像中的具体位置\n",
    "    2. 局部性(Locality): 图像中的对象识别关注的是局部区域, 而不是整个图像\n",
    "- 从MLP到CNN(数学表示):\n",
    "    1. 已知二维输入图像$X_{i, j}$, 则MLP第一个隐藏层(特征图)可表示为:$$H_{i, j} = U_{i, j} + \\sum_k \\sum_l W_{i, j}^{k, l} X_{k, l}$$\n",
    "    2. 使用($k=i+a$, $l=j+b$)重新索引下标($k, l$): $$H_{i, j} = U_{i, j} + \\sum_a \\sum_b V_{i, j}^{a, b} X_{i+a, j+b}$$\n",
    "    3. 由平移不变性, 检测对象在输入图像中的平移反映在特征图中的平移, 即$U_{i, j}$和$V_{i, j}^{a, b}$不依赖于$(i, j)$: $$H_{i, j} = u + \\sum_a \\sum_b V^{a, b} X_{i+a, j+b}$$\n",
    "    Note:\n",
    "       - 这就是卷积的数学表达, $V^{a, b}$称为卷积核, 即卷积层的权重, 是可学习的参数\n",
    "       - $V^{a, b}$比$V_{i, j}^{a, b}$大幅缩减参数规模, 通过对象在图像中的平移不变性\n",
    "    4. 由局部性原则, 卷积区域比图像小, 由此可以继续改写为: $$H_{i, j} = u + \\sum_{a=-\\Delta}^{\\Delta} \\sum_{b=-\\Delta}^{\\Delta} V^{a, b} X_{i+a, j+b}$$\n",
    "    Note:\n",
    "        - 其中$\\Delta$是卷积核的半宽, 即5x5的卷积核, 半宽为2\n",
    "- 卷积与互相关\n",
    "    1. 数学中的卷积: $$(f*g)(x) = \\int f(z)g(x-z)dz$$\n",
    "        (对于图像离散化)$$(f*g)(i, j) = \\sum_a \\sum_b f(a, b)g(i-a, j-b)$$\n",
    "    2. 数学中的互相关: $$(f*g)(x) = \\int f(z)g(x+z)dz$$\n",
    "        (对于图像离散化)$$(f*g)(i, j) = \\sum_a \\sum_b f(a, b)g(i+a, j+b)$$\n",
    "    3. 卷积和互相关是非常类似的操作, 差别在于是否反转kernal\n",
    "    4. CNN中使用的操作实际上是计算互相关\n",
    "- 通道(channel): 对于图像, 通道表示颜色信息的维度. 对于RGB图像, 通道数为3; 灰度图像, 通道数为1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7f7bc9",
   "metadata": {},
   "source": [
    "---\n",
    "# 卷积层\n",
    "- 严格来讲, 卷积层是错误的叫法, 实际表达的运算是互相关\n",
    "- 但两者差别只在于卷积需要水平和垂直翻转二维卷积核张量\n",
    "- 由于卷积核是从数据中学习得到的, 因此无论采用卷积还是互相关, 卷积层的输出不会受到影响\n",
    "- 在卷积层中, 输入张量和核张量通过互相关运算, 然后添加偏置标量生成输出张量\n",
    "- Feature Map: 卷积层也称特征映射\n",
    "- 元素: 卷积核张量上的每一个权重称为元素\n",
    "- 感受野(receptive field): 对于某一层的任意元素x, 其感受野指在向前传播期间可能影响x计算的所有元素(来自所有之前层)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a84665",
   "metadata": {},
   "source": [
    "## 定义互相关操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94597974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19., 25.],\n",
      "        [37., 43.]])\n"
     ]
    }
   ],
   "source": [
    "def coor2d(X, K):\n",
    "    \"\"\"\n",
    "    定义互相关运算\n",
    "    \"\"\"\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = torch.sum(X[i:i + h, j:j + w] * K)\n",
    "    return Y\n",
    "\n",
    "X = torch.tensor([\n",
    "    [0, 1, 2], \n",
    "    [3, 4, 5], \n",
    "    [6, 7, 8]\n",
    "])\n",
    "K = torch.tensor([\n",
    "    [0, 1],\n",
    "    [2, 3]\n",
    "])\n",
    "Y = coor2d(X, K)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e6e355",
   "metadata": {},
   "source": [
    "## 定义卷积层\n",
    "- 卷积层的两个被训练参数: 卷积核, 标量偏置\n",
    "- 高度h和宽度w的卷积核称为hxw卷积核, 带有hxw卷积核的卷积层, 称为hxw卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f907477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "    def forward(self, x):\n",
    "        return coor2d(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c748599",
   "metadata": {},
   "source": [
    "## 卷积层的应用举例: 边缘检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a48cbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 定义测试图像\n",
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11b9b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造卷积核, 水平两个元素相同输出0, 否则非0\n",
    "K = torch.tensor([[1, -1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23f91b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = coor2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2292e5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coor2d(X.T, K)  # 这个卷积核只能检测垂直边缘, 不能检测水平边缘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4777fe66",
   "metadata": {},
   "source": [
    "## 学习卷积核\n",
    "- 已知输入X和输出Y, 通过训练, 学习出这个卷积核"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad4eb2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 6.078\n",
      "epoch 2, loss 3.033\n",
      "epoch 3, loss 1.590\n",
      "epoch 4, loss 0.874\n",
      "epoch 5, loss 0.500\n",
      "epoch 6, loss 0.296\n",
      "epoch 7, loss 0.180\n",
      "epoch 8, loss 0.111\n",
      "epoch 9, loss 0.069\n",
      "epoch 10, loss 0.044\n",
      "epoch 11, loss 0.028\n",
      "epoch 12, loss 0.018\n",
      "epoch 13, loss 0.011\n",
      "epoch 14, loss 0.007\n",
      "epoch 15, loss 0.005\n",
      "epoch 16, loss 0.003\n",
      "epoch 17, loss 0.002\n",
      "epoch 18, loss 0.001\n",
      "epoch 19, loss 0.001\n",
      "epoch 20, loss 0.000\n",
      "Final kernel: tensor([[ 0.9976, -1.0022]])\n"
     ]
    }
   ],
   "source": [
    "# 构造卷积层\n",
    "conv2d = nn.Conv2d(1, 1, kernel_size=(1, 2), bias=False)\n",
    "\n",
    "# 使用4维输入和输出格式(批量大小, 通道, 高度, 宽度)\n",
    "X = X.reshape((1, 1, 6, 8))\n",
    "Y = Y.reshape((1, 1, 6, 7))\n",
    "lr = 0.03\n",
    "\n",
    "# 训练卷积层\n",
    "for i in range(20):\n",
    "    Y_hat = conv2d(X)\n",
    "    loss = (Y_hat - Y) ** 2\n",
    "    conv2d.zero_grad()\n",
    "    loss.sum().backward()\n",
    "    \n",
    "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
    "    print(f\"epoch {i + 1}, loss {loss.sum():.3f}\")\n",
    "\n",
    "print(f\"Final kernel: {conv2d.weight.data.reshape(1, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbd5e8e",
   "metadata": {},
   "source": [
    "---\n",
    "# 填充和步幅\n",
    "- 填充(padding): 解决卷积丢失图像边缘信息的有效方法\n",
    "- 步幅(stride): 原始图像的分辨率过高, 希望大幅减少图像的高度和宽度时采用的方法\n",
    "- 卷积的输出形状取决于输入形状和卷积核形状\n",
    "- 假设输入形状$n_h \\times n_w$, 卷积核形状$k_n \\times k_w$, 则卷积后的输出形状为($n_h - k_n + 1$, $n_w - k_w + 1$)\n",
    "- 很少使用不一致的步幅和填充"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ce16c8",
   "metadata": {},
   "source": [
    "## 填充\n",
    "- 只有使用1x1的卷积核, 卷积后的形状才与原图像形状相同\n",
    "- 卷积核约大, 丢失像素越多\n",
    "- 连续卷积多次后, 累积丢失的像素数就会增多\n",
    "- 填充: 在输入图像的边缘填充0, 假设填充$p_h$行, $p_w$列, 则输出形状($n_h + p_h - k_n + 1$, $n_w + p_w - k_w + 1$)\n",
    "- 通常使用`奇数`的卷积核大小\n",
    "- 当卷积核或输入图像的形状不是正方形时, 可以填充不同的数值, 使输出形状是正方形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cab15bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def comp_conv2d(conv2d, X):\n",
    "    X = X.reshape((1, 1) + X.shape)  # 这里的(1, 1)表示批量大小和通道数\n",
    "    Y = conv2d(X)\n",
    "    return Y.reshape(Y.shape[2:])\n",
    "\n",
    "conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1)\n",
    "X = torch.rand(size=(8, 8))\n",
    "comp_conv2d(conv2d, X).shape  # 输出形状为(8, 8), 因为padding=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e7d61ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=(5, 3), padding=(2, 1))\n",
    "comp_conv2d(conv2d, X).shape  # 输出形状为(8, 8), 因为padding=(2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60727104",
   "metadata": {},
   "source": [
    "## 步幅\n",
    "- 计算互相关时, 卷积窗口默认向下、向右滑动1个元素\n",
    "- 为了高效计算或缩减采样次数, 卷积窗口可以跳过一些元素, 每次滑动的元素数量称为步幅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5415256c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2)\n",
    "comp_conv2d(conv2d, X).shape  # 输出形状为(4, 4), 因为stride=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aea99610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\n",
    "comp_conv2d(conv2d, X).shape  # 输出形状为(3, 2), 因为stride=(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bcd677",
   "metadata": {},
   "source": [
    "---\n",
    "# 多输入多输出通道"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
